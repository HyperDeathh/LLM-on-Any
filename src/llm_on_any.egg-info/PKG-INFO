Metadata-Version: 2.4
Name: llm-on-any
Version: 0.1.0
Summary: Cross-platform mobile-friendly CLI to list, download, and run open-source LLMs (Android Termux + iOS via Pythonista/Shortcuts).
Author: Your Name
License: MIT
Keywords: llm,mobile,android,ios,termux,offline
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests>=2.31.0
Requires-Dist: tqdm>=4.66.0
Requires-Dist: typer>=0.12.0
Requires-Dist: rich>=13.7.0
Requires-Dist: huggingface_hub>=0.21.4
Provides-Extra: llama
Requires-Dist: llama-cpp-python>=0.2.90; extra == "llama"
Provides-Extra: hf
Requires-Dist: transformers>=4.41.0; extra == "hf"
Requires-Dist: accelerate>=0.30.0; extra == "hf"
Dynamic: license-file

# LLMonAny (LLM on Any)

A modern, open-source CLI to list, download, and run open-weight LLMs on any device, with a mobile-first mindset.

- Android: Termux (recommended)
- Windows/macOS/Linux: Python 3.9+
- iOS: Python-only is limited; a native Swift + llama.cpp (Metal) bridge is planned

Support: https://buymeacoffee.com/hyperdeath — Discord: rhaegarrr — Please open GitHub Issues for questions.

## Features

- Model list and download status (Downloaded/Not downloaded)
- Download by number and start chatting immediately
- Menu flow: New chat, Past chats, Settings
- Per-model settings: max_tokens, temperature, top_k, top_p, ctx_size, threads, system_prompt
- Chat history persisted as JSON

## Install (Desktop)


## Platform guides

Below are platform-specific steps. Prefer GGUF + llama.cpp for mobile. Use Transformers for original/safetensors (heavier, requires torch).

### Windows 10/11 (PowerShell)

```powershell
# 1) Create venv and install
python -m venv .venv ; .\.venv\Scripts\Activate.ps1
pip install -e .

# 2) GGUF (llama.cpp)
pip install -e .[llama]

# 3) Transformers (optional)
pip install -e .[hf]
pip install torch --index-url https://download.pytorch.org/whl/cpu

# 4) Use CLI
lom list
lom download 1
lom chat 1 -p "Hello"

# Optional: Set data dir
$env:LOM_HOME = "$env:USERPROFILE\\.llmonany"
```

Notes:
- GGUF models download to `%USERPROFILE%\\.llmonany\\models` by default.
- For gated HF repos, accept terms and `huggingface-cli login` first.

### Linux (Ubuntu/Debian/Fedora/Arch)

```bash
```powershell
python -m venv .venv ; .\.venv\Scripts\Activate.ps1
pip install -e .
# Local inference (GGUF via llama.cpp, optional):
pip install -e .[llama]

# Original safetensors (Transformers, optional):
pip install -e .[hf]
# Torch kurulumu platforma özeldir; CPU için örnek:
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

## Quickstart

```powershell
lom list
lom download 3
lom download 3 -i "model.safetensors,tokenizer.json,config.json"
lom download --id gpt-oss-20b
lom chat 3
# or a quick single message
lom chat 3 -p "Hello there!"
 # delete a downloaded model
 lom delete 3
```

## Android (Termux)

### Android (Termux)

```bash

```bash
pkg update -y && pkg upgrade -y
pkg install -y python clang cmake git
python -m venv .venv && source .venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install llm-on-mobile
# Local inference (may compile):
pip install llama-cpp-python || echo "Skipping"

# Use
lom list
lom download 3
lom chat 3
```

Notes
- On some devices `llama-cpp-python` compiles from source; we’ll add a direct `llama.cpp` binary adapter next.
- Data dir: `$LOM_HOME` (default `~/.llmonany`). Models live under `models/`.
 - Original/safetensors modeller için Transformers yolunu kullanın (bkz. Install bölümü). Gated repo’lar için önce HF’de şartları kabul edin.

## Commands

- `lom help` — Detailed help and examples
- `lom list` — List models and status
- `lom download <n>` — Download model `n`
- `lom download --id ID` — Download model by id
- `lom download <n> -i "pattern1,pattern2"` — Limit HF snapshot to selected files
- `lom delete <n>` — Delete downloaded model `n`
- `lom chat <n>` — Menu (New chat / Past chats / Settings)
- `lom chat <n> -p "..."` — Single prompt without menu

Notes:
- Building `llama-cpp-python` can take time; that’s expected.
- Prefer quantized GGUF models for speed and memory.
- For gated HF repos, accept terms and `huggingface-cli login`.
- Multi-file HF hubs can be very large; use `--include` to filter (e.g., `model.safetensors,tokenizer.json,config.json`).
- Uyarı: `*.safetensors` tüm shard'ları eşleştirir ve çok büyük indirmeler başlatır.

### iPadOS / iOS

- Native local inference with llama-cpp/torch isn’t officially supported.
- Options:
	- Use an Android device (Termux) or a desktop machine.
	- Advanced: shell apps (e.g., a-Shell) with Python may run tiny Transformers models, but setup is non-trivial and unsupported.
- You can still sync `$LOM_HOME` for registry/config/history across devices.

### HarmonyOS

- If a compatible terminal + Python environment exists (often Android-compatible), follow the Android (Termux) steps.
- Without Termux, use an alternative terminal/python app that supports pip and compiling; prefer GGUF.
- Device constraints vary; Transformers + torch may not be practical.

### macOS (optional)

```bash
- `lom registry` — Registry source priority
- `lom doctor` — Environment check
- `lom paths` — Important paths

## Model Registry

Priority:
1) `LOM_REGISTRY` (file path)
2) `$LOM_HOME/models.json`
3) `./models.json`
4) Built-in defaults

Example `models.json` entry:

```json
[
	{
		"id": "llama-3.1-8b-q4",
		"name": "Llama 3.1 8B Instruct Q4_0",
		"size": "~4.5 GB",
		"family": "llama",
		"format": "gguf",
		"url": "https://…/model.gguf?download=true",
		"sha256": null
	}
]
```

### About the provided models

You shared these hubs:
- xai-org/grok-2
- meta-llama/Llama-3.3-70B-Instruct
- meta-llama/Llama-4-Scout-17B-16E-Instruct
- meta-llama/Llama-4-Maverick-17B-128E-Instruct
- meta-llama/Llama-3.1-8B-Instruct
- openai/gpt-oss-120b
- openai/gpt-oss-20b

Notes:
- GGUF modeller (QuantFactory, TheBloke, bartowski vb.) doğrudan `lom download` ile iner ve llama.cpp runner ile çalışır.
- Orijinal/safetensors modelleri için `lom download` HF snapshot indirir ve Transformers runner ile çalıştırır. Bunun için `pip install -e .[hf]` ve uygun `torch` gerekir.
- Many Meta Llama hubs are gated; accept terms and login on HF before downloading.

We will include them in `models.json` as either direct URLs (if available) or as HF repo + filename. If gated, `lom download` will guide you to use `huggingface-cli` after accepting terms.

## Multi-device use

- Point `LOM_HOME` to a shared location to re-use config/history across devices.
- Models are large; consider per-device download or external storage.

## Roadmap

- Android `llama.cpp` binary adapter (no Python binding)
- Minimal HTTP server mode (mobile browser chat)
- Model/device capability hints
- CI/CD and PyPI release workflow

## Support & Contact

- Buy Me a Coffee: https://buymeacoffee.com/hyperdeath
- Discord: rhaegarrr
- GitHub Issues: please open an issue for questions/bugs/ideas.

## License

MIT
